{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02d132bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5659fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7994b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26b8cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/kanha_122mm0924.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64c7250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "1 \n",
      "\n",
      "<class 'llama_index.core.schema.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38e357ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3385\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "print(len(document.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4460379",
   "metadata": {},
   "source": [
    "WINDOW SENTENCE RETRIEVAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30d6cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=5,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7db815a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "llm = Groq(model=\"llama3-70b-8192\", temperature=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95679918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding \n",
    "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea1343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bba23c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "# Apply settings globally (recommended by LlamaIndex)\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "Settings.node_parser = node_parser\n",
    "\n",
    "# Define index path\n",
    "index_path = \"./sentence_index\"\n",
    "\n",
    "# Check if index exists\n",
    "if not os.path.exists(index_path):\n",
    "    # Build index and persist\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir=index_path)\n",
    "else:\n",
    "    # Load index from storage\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=index_path)\n",
    "    index = load_index_from_storage(storage_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d37f0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "postproc = MetadataReplacementPostProcessor(\n",
    "    target_metadata_key=\"window\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dc90490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# BAAI/bge-reranker-base\n",
    "# link: https://huggingface.co/BAAI/bge-reranker-base\n",
    "rerank = SentenceTransformerRerank(\n",
    "    top_n=2, model=\"BAAI/bge-reranker-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "974a1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_engine = index.as_query_engine(\n",
    "    similarity_top_k=6, node_postprocessors=[postproc, rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6241802",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_response = sentence_window_engine.query(\n",
    "    \"what is kanhaiya's educational qualification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "002ec867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kanhaiya's educational qualifications are Bachelor of Technology in Metallurgical and Materials Engineering with a CGPA of 7.24 from National Institute of Technology, Rourkela, and he is currently in Diploma Level of B.Sc. in Data Science from Indian Institute of Technology, Madras.\n"
     ]
    }
   ],
   "source": [
    "print(window_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef61e87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec8332ab",
   "metadata": {},
   "source": [
    "PUTTING EVERYTHING TOGETHER AND USING LLM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22017684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "\n",
    "def build_sentence_window_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index\"\n",
    "):\n",
    "    # Set up node parser\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=sentence_window_size,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "\n",
    "    # Use HuggingFace for embedding\n",
    "    embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "    # Apply global settings (replaces ServiceContext)\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = embedding_model\n",
    "    Settings.node_parser = node_parser\n",
    "\n",
    "    # Load or build index\n",
    "    if not os.path.exists(save_dir):\n",
    "        index = VectorStoreIndex.from_documents(documents)\n",
    "        index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=save_dir)\n",
    "        index = load_index_from_storage(storage_context)\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "def get_sentence_window_query_engine(index, similarity_top_k=6, rerank_top_n=2):\n",
    "    # Postprocessors\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n,\n",
    "        model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    return index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k,\n",
    "        node_postprocessors=[postproc, rerank]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d62230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./sentence_index\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./sentence_index\\index_store.json.\n"
     ]
    }
   ],
   "source": [
    "index = build_sentence_window_index(\n",
    "    [document],\n",
    "    llm=llm,\n",
    "    save_dir=\"./sentence_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ed7b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = get_sentence_window_query_engine(index, similarity_top_k=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "395b94f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a resume of Kanhaiya Goyal, showcasing his education, technical skills, relevant coursework, and work experience as a full-stack developer intern at VegaPro.ai.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\". explain in brief\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f41795f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query seems to be unrelated to the provided context information. However, I'll attempt to provide a response based on the context.\n",
      "\n",
      "Since the context information is related to a person's education, technical skills, and experience in the field of data science and web development, it's challenging to provide a direct answer to the query about nanomaterials fabrication limitations.\n",
      "\n",
      "However, considering the person's educational background in Metallurgical and Materials Engineering, it's possible to provide a general response.\n",
      "\n",
      "In the field of nanomaterials fabrication, some common limitations include:\n",
      "1. **Scalability**: Fabricating nanomaterials at a large scale while maintaining their unique properties is a significant challenge.\n",
      "2. **Uniformity**: Achieving uniformity in the size, shape, and properties of nanomaterials is crucial, but it can be difficult to control.\n",
      "3. **Interfacial issues**: The interface between nanomaterials and other materials can lead to issues such as agglomeration, which affects their properties and performance.\n",
      "4. **Toxicity and safety concerns**: The potential toxicity and safety concerns associated with handling and using nanomaterials need to be carefully addressed.\n",
      "5. **Characterization challenges**: Characterizing nanomaterials' properties and behavior at the nanoscale can be a complex task, requiring specialized equipment and expertise.\n",
      "\n",
      "Please note that this response is not directly related to the context information and is a general attempt to provide an answer based on the person's educational background.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\" What types of limitations are faced during the nanomaterials fabrication? Discuss each one of them in brief.   \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fb33bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, Kanhaiya has the following skills:\n",
      "\n",
      "* Programming Languages: C/C++, JavaScript\n",
      "* Frameworks: NodeJs, ExpressJs, ReactJs, NextJs, Tailwind CSS, shadCnc, Redux, streamlit\n",
      "* Databases: PostgreSQL, MongoDB, MySql\n",
      "* Developer Tools: Git, GitHub, Docker, VS Code, Figma, Adobe Illustrator, Canva\n",
      "* Libraries: scikit-learn, tensorflow, light-bgm\n",
      "* Relevant Coursework: Data Structures, PowerBi, PostgreSQL, Statistics, Machine Learning, Web Development, Exploratory Data Analysis\n",
      "\n",
      "Regarding his experience, Kanhaiya has worked as a Full Stack Developer Intern at VegaPro.ai from May 2025 to present. During this period, he developed a full-fledged poultry management web application with production-level architecture, built the frontend using Next.js, and designed and implemented the backend in Flask. He also utilized PostgreSQL for scalable data storage, optimizing queries to enhance performance, and managed core admin functionalities.\n",
      "\n",
      "As for whether he is a good fit for a cloud management role, Kanhaiya has experience with cloud-related technologies such as Docker and has worked on developing scalable data storage solutions using PostgreSQL. Additionally, his experience in developing full-stack applications and managing core admin functionalities demonstrates his ability to handle operational oversight. However, it is essential to note that cloud management requires specific skills and knowledge, and Kanhaiya's experience may not directly translate to a cloud management role. Further evaluation would be necessary to determine his fit for such a position.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\" what are the skills that kanhaiya has? and explain me about his experience and is he good fit for a cloud management roll  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0128f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
